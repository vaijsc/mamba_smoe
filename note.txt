DistributedDataParallel(
  (module): TransformerSeq(
    (in_emb): Embedding(267735, 128)
    (out_emb): Linear(in_features=128, out_features=267735, bias=True)
    (layers): ModuleList(
      (0-2): 3 x TransformerSeqLayer(
        (attn): MultiHeadSeqAttention(
          (attn): SeqAttention(
            (dropout): Dropout(p=0.7, inplace=False)
          )
          (proj_query): Linear(in_features=128, out_features=128, bias=False)
          (proj_out): Linear(in_features=128, out_features=128, bias=False)
          (proj_val): Linear(in_features=128, out_features=128, bias=False)
          (proj_key): Linear(in_features=128, out_features=128, bias=False)
        )
        (smoe): CustomizedMoEPositionwiseFF(
          (gate): CustomNaiveGate_Balance_SMoE(
            (gate): Linear(in_features=128, out_features=16, bias=True)
          )
          (experts): _Expert(
            (htoh4): FMoELinear(num_expert=16, in_features=128,         out_features=128, bias=True, rank=0)
            (h4toh): FMoELinear(num_expert=16, in_features=128,         out_features=128, bias=True, rank=0)
            (activation): Sequential(
              (0): ReLU()
              (1): Dropout(p=0.7, inplace=False)
            )
          )
          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.7, inplace=False)
        )
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
** trainer_params 
{'batch_size': 16, 'batch_split': 2, 'nb_batches_per_iter': 1000, 'nb_iter': 60, 
'checkpoint_path': '/home/ubuntu/workspace/MomentumSMoE/result/checkpoints/mamba_smoe.pt', 'resume': False, 
'pretrained_weight': '', 'full_eval_mode': False}
** 
ipdb> train_data.shape
torch.Size([16, 6451688])
ipdb> val_data.shape
torch.Size([16, 13602])
ipdb> test_data.shape
torch.Size([16, 15348])
** data_params 
{'data_path': '/home/ubuntu/workspace/dataset/wikitext-103', 'data_name': 'wikitext-103', 'vocab_size': 267735}
** env_params
{'distributed': True, 'local_rank': 0, 'rank': 0, 'world_size': 1, 'device': device(type='cuda')}
** model_params
{'hidden_size': 128, 'inner_hidden_size': 128, 'nb_layers': 3, 'block_size': 256, 'nb_heads': 8, 'attn_span': 256, 
'dropout': 0.7, 'architecture': 'sgsgsg', 'base_arch': 'transformer', 'smoe_dropout': False, 'optimal_policy': False, 
'load_balance': 0.01, 'moe_top_k': 2, 'freq': 0.03, 'freq_type': 'fix', 'alpha': 1.0, 'gate_name': 'smoe', 
'act_experts': 'shuffle', 'g_blance': False, 'opt_blance': False, 'combine_gate': False, 'opt_loss': 'mse', 
'gamma1': 1.0, 'gamma2': 1.0, 'mu': 0.9, 'beta1': 0.9, 'beta2': 0.999}
** adapt_span_params
{'adapt_span_enabled': False, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0, 'adapt_span_cache': False}
** optim_params
{'lr': 0.0007, 'momentum': 0.9, 'optim': 'adam', 'lr_warmup': 3000, 'grad_clip': 0}
** wandb_params
{'project_name': 'project_name', 'job_name': 'job_name', 'wandb_flag': False}
** optimizer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0007
    lr: 0.0
    maximize: False
    weight_decay: 1e-05
)
Total of Parameters: 70603015
Total of Trainable Parameters: 70603015
hid_cache = [hid_cache[0] , hid_cache[1]]
hid_cache[i] = [hid_cache[i][0], hid_cache[i][1], hid_cache[i][2]]
hid_cache[i][j] # torch.Size([16, 256, 128])
nb_batches_per_iter = 1000

------------> ipdb> numpy.sum(train_data[0][:100000].to('cpu').detach().numpy() >= 1000)
41213

------------> nb_batches_per_iter_max = 1000
"""
X = data[:, train_pos : train_pos + block_size].contiguous()
Y = data[:, train_pos + 1 : train_pos + block_size + 1].contiguous()
ipdb> X.shape
torch.Size([16, 256])
ipdb> Y.shape
torch.Size([16, 256])
"""
# load_balance 0.01
------------------> *** train step *** <------------------
X torch.Size([8, 256])
h_cache [torch.Size([8, 256, 128]), torch.Size([8, 256, 128]), torch.Size([8, 256, 128])]
out torch.Size([2048, 267735])

# thay đổi moe_top_k = 1 -> thay đổi ở config
self.arch 'sgsgsg'

"""
ipdb> moe_inp.shape
torch.Size([2048, 128])
"""

** configuration of smoe
input shape torch.Size([2048, 128]) <- 8 * 256, 128
self.num_expert = 16
self.d_model = 128
self.world_size = 1
ipdb> self.experts
_Expert(
  (htoh4): FMoELinear(num_expert=16, in_features=128,         out_features=128, bias=True, rank=0)
  (h4toh): FMoELinear(num_expert=16, in_features=128,         out_features=128, bias=True, rank=0)
  (activation): Sequential(
    (0): ReLU()
    (1): Dropout(p=0.7, inplace=False)
  )
)
self.experts_fused = True
self.gate = CustomNaiveGate_Balance_SMoE(
  (gate): Linear(in_features=128, out_features=16, bias=True)
)
ipdb> print(self.gate_hook)
None
ipdb> print(self.mask)
None
ipdb> print(self.mask_dict)
None
ipdb> print(self.moe_group)
None
ipdb> moe_inp_batch_size
[2048]
ipdb> self.slice_size
1

# Compute the L2 norm in smaller steps to reduce memory usage
norms = torch.norm(moe_inp, p=2, dim=-1, keepdim=True)

# Normalize the tokens
moe_inp = moe_inp / norms  # Element-wise division

# Scale moe_inp (keeping this operation out-of-place)
moe_inp = moe_inp * (1/3)  # Element-wise multiplication

# Compute the similarity matrix
similarity_matrix = torch.matmul(moe_inp, moe_outp.transpose(1, 2))

# Use the lower triangular part of the similarity matrix
similarity_matrix = torch.tril(similarity_matrix)

# Use the similarity matrix to update moe_outp (out-of-place operation)
moe_outp = torch.matmul(similarity_matrix, moe_inp)

dep trai 10d